plot(density(pmg$bill_depth_mm))
?shapiro.test
plot(density(pmg$bill_depth_mm))
z <- qqnorm(my.penguins$bill_depth_mm, pch = 1, frame = FALSE)
z <- qqline(my.penguins$bill_depth_mm, col = "blue", lwd = 2)
?shapiro.test
library(MASS)
fitdistr(pmg$bill_depth_mm, "normal")
mean(pmg$bill_depth_mm)
sd(pmg$bill_depth_mm)
source("~/Documents/Github/Newcastle Workbook/R_practical_Oct.R", echo=TRUE)
t.test(pmg$bill_depth_mm~pmg$sex, var.equal = T)
t.test(pmg$bill_depth_mm~pmg$sex, var.equal = T)
library(MASS)
fitdistr(pmg$bill_depth_mm, "normal")
mean(pmg$bill_depth_mm)
sd(pmg$bill_depth_mm)
source("~/Documents/Github/Newcastle Workbook/R_practical_Oct.R")
t.test(pmg$bill_depth_mm~pmg$sex, var.equal = T)
#Install all supporting packages
install.packages("mlbench")
library(mlbench)
library(ggplot2)
library(bestglm)
library(glmnet)
library(MASS)
library(GGally)
data(BreastCancer)
## Check size
dim(BreastCancer)
head(BreastCancer)
#About the Dataset
?BreastCancer
summary(BreastCancer)
#Omitted Data and structure
bc_data = na.omit(BreastCancer)
dim(bc_data)
summary(bc_data)
str(bc_data)
#Basic EDA and plots
cancer_count = table(bc_data$Class)
barplot(cancer_count)
pie(cancer_count)
#basic EDA beautified
ggplot(bc_data, aes(bc_data$Class, , fill=Class)) +
geom_bar( ) +
scale_fill_hue(c = 60) +
theme(legend.position = "none") + theme_bw() +
labs(x = "Cancer type", title = "Cancer Count of Benign and Malignant", y = "Count")
#EDA scatterplot
ggplot(bc_data, aes(x=bc_data$Cell.size, y=bc_data$Cell.shape, color=Class)) +
geom_point(size=3, alpha = 0.5) + theme_bw() +
labs(x = "Cell Size", title = "Cancer Cell type", y = "Cell Shape")
#EDA of the whole dataframe with pair plots
data(bc_data)
ggpairs(bc_data, columns = 2:10, ggplot2::aes(colour=Class))
#Changing Ordinal to Numeric
bc_data$Cl.thickness= as.numeric(bc_data$Cl.thickness)
bc_data$Cell.size= as.numeric(bc_data$Cell.size)
bc_data$Cell.shape= as.numeric(bc_data$Cell.shape)
bc_data$Marg.adhesion= as.numeric(bc_data$Marg.adhesion)
bc_data$Epith.c.size= as.numeric(bc_data$Epith.c.size)
bc_data$Bare.nuclei= as.numeric(bc_data$Bare.nuclei)
bc_data$Bl.cromatin= as.numeric(bc_data$Bl.cromatin)
bc_data$Normal.nucleoli= as.numeric(bc_data$Normal.nucleoli)
bc_data$Mitoses= as.numeric(bc_data$Mitoses)
#Remove the column ID
bc_no_id = subset(bc_data, select = -c(Id))
#Applying Logistic regression
#AIC and BIC
bss_AIC = bestglm(bc_no_id, family=binomial, IC="AIC")
bss_BIC = bestglm(bc_no_id, family=binomial, IC="BIC")
#Checking the results of the subsets
bss_AIC$Subsets
bss_BIC$Subsets
#Identifying the best fitting models
(best_AIC = bss_AIC$ModelReport$Bestk)
(best_BIC = bss_BIC$ModelReport$Bestk)
#Multiple graphs
par(mfrow=c(1,2))
p = ncol(bc_no_id[1:ncol(bc_no_id)-1])
#Plotting graphs and highlighting the value of k
plot(0:p, bss_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
#AIC and BIC best fit model with 5 values
pstar = 5
## Check which predictors are in the 1-predictor model
bss_AIC$Subsets[pstar+1,]
## Intercept Lag1 Lag2 Lag3 Lag4 Lag5 Volume logLikelihood AIC
## 1 TRUE FALSE TRUE FALSE FALSE FALSE FALSE -745.2093 1492.419
## Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_AIC$Subsets[pstar+1, 2:(p+1)]))
## [1] FALSE TRUE FALSE FALSE FALSE FALSE
cancer_data = data.frame(bc_no_id[,indices])
## Obtain regression coefficients for this model
logreg1_fit = glm(Class ~ ., data=cancer_data, family="binomial")
summary(logreg1_fit)
#Rough plot for the idea of the best fit line
plot(logreg1_fit)
ggpairs(bc_data, columns = 2:10, ggplot2::aes(colour=Class))
lda_fit = lda(Class ~ ., data = bc_no_id)
lda_fit
plot(lda_fit)
lda_fit
lda_fit = lda(Class ~ ., data = bc_no_id)
lda_fit
class_cv = function(bc_no_id, cancer_data, fold_ind, model_type, optim_lambda){
nfolds = max(fold_ind) # number of folds...
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.") # validating partitions...
cv_errors=numeric(nfolds) # empty list of size equal to number of folds...
for(fold in 1:nfolds) # fitting different models based on model_type for each fold and making prediction then finding yhat(predicted class)...
{
if(model_type == 'IC'){ # Logistic regression using features selected from subset selection AIC and BIC...
tmp_fit = glm(Class ~ ., data = cancer_data[fold_ind != fold,], family = "binomial")
pred = predict(tmp_fit, cancer_data[fold_ind == fold, 1:ncol(cancer_data)-1])
yobs = cancer_data$Class[fold_ind == fold]
yhat = ifelse(pred > 0.5, 'malignant', 'benign')
}
else if(model_type == 'LASSO'){ # LASSO Regularized Logistic Regression...
tmp_fit = glmnet(bc_no_id[fold_ind != fold, 1:9], bc_no_id[fold_ind != fold, 10], alpha = 1, family = "binomial", lambda = optim_lambda)
pred = predict(tmp_fit, as.matrix(bc_no_id[fold_ind == fold, 1:9]))
yobs = bc_no_id$Class[fold_ind == fold]
yhat = ifelse(pred > 0.5, 'malignant', 'benign')
}
else if(model_type == 'LDA'){ # Linear discriminant analysis...
tmp_fit = lda(Class ~ ., data = bc_no_id[fold_ind != fold,])
pred = predict(tmp_fit, bc_no_id[fold_ind == fold, 1:9])
yobs = bc_no_id$Class[fold_ind==fold]
yhat = ifelse(pred$x > 0.5, 'malignant', 'benign')
}
cv_errors[fold] = 1 - mean(yobs == yhat) # Calculate the training error
fold_sizes = numeric(nfolds) # placeholder for length of each fold...
for(fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold)) # calculating length of each fold...
test_error = weighted.mean(cv_errors, w = fold_sizes) # calculating weighted mean of the cross validation errors...
return(test_error)
}
model_type = c('IC', 'LASSO', 'LDA')
error_list = c()
for(i in model_type){
error = class_cv(bc_no_id, cancer_data, fold_index, i, lasso_cv_value$lambda.min) # performing cross validation on each type of models
error_list = append(error_list, error)
}
model_name = c('Logistic Regression with AIC, BIC', 'Logistic Regression with LASSO regularization', 'Linear Discriminant Analysis')
df_error = data.frame(model_name, error_list)
colnames(df_error) = c("Model", "Error Rate")
df_error
setwd("~/Documents/Github/Newcastle Workbook/Stats_Learning")
n = nrow(bc_no_id) - 1
# 10-fold cross validation...
nfolds = 10
# Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
# Print first few fold-assignments
head(fold_index)
class_cv = function(bc_no_id, cancer_data, fold_ind, model_type, optim_lambda){
nfolds = max(fold_ind) # number of folds...
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.") # validating partitions...
cv_errors=numeric(nfolds) # empty list of size equal to number of folds...
for(fold in 1:nfolds) # fitting different models based on model_type for each fold and making prediction then finding yhat(predicted class)...
{
if(model_type == 'IC'){ # Logistic regression using features selected from subset selection AIC and BIC...
tmp_fit = glm(Class ~ ., data = cancer_data[fold_ind != fold,], family = "binomial")
pred = predict(tmp_fit, cancer_data[fold_ind == fold, 1:ncol(cancer_data)-1])
yobs = cancer_data$Class[fold_ind == fold]
yhat = ifelse(pred > 0.5, 'malignant', 'benign')
}
else if(model_type == 'LASSO'){ # LASSO Regularized Logistic Regression...
tmp_fit = glmnet(bc_no_id[fold_ind != fold, 1:9], bc_no_id[fold_ind != fold, 10], alpha = 1, family = "binomial", lambda = optim_lambda)
pred = predict(tmp_fit, as.matrix(bc_no_id[fold_ind == fold, 1:9]))
yobs = bc_no_id$Class[fold_ind == fold]
yhat = ifelse(pred > 0.5, 'malignant', 'benign')
}
else if(model_type == 'LDA'){ # Linear discriminant analysis...
tmp_fit = lda(Class ~ ., data = bc_no_id[fold_ind != fold,])
pred = predict(tmp_fit, bc_no_id[fold_ind == fold, 1:9])
yobs = bc_no_id$Class[fold_ind==fold]
yhat = ifelse(pred$x > 0.5, 'malignant', 'benign')
}
cv_errors[fold] = 1 - mean(yobs == yhat) # Calculate the training error
}
# Calculating mean error of all the folds...
fold_sizes = numeric(nfolds) # placeholder for length of each fold...
for(fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold)) # calculating length of each fold...
test_error = weighted.mean(cv_errors, w = fold_sizes) # calculating weighted mean of the cross validation errors...
return(test_error)
}
model_type = c('IC', 'LASSO', 'LDA')
error_list = c()
for(i in model_type){
error = class_cv(bc_no_id, cancer_data, fold_index, i, lasso_cv_value$lambda.min) # performing cross validation on each type of models
error_list = append(error_list, error)
}
model_name = c('Logistic Regression with AIC, BIC', 'Logistic Regression with LASSO regularization', 'Linear Discriminant Analysis')
df_error = data.frame(model_name, error_list)
colnames(df_error) = c("Model", "Error Rate")
df_error
library(mlbench)
library(ggplot2)
library(bestglm)
library(glmnet)
library(MASS)
library(GGally)
## Load the data
data(BreastCancer)
## Check size
dim(BreastCancer)
head(BreastCancer)
#About the Dataset
?BreastCancer
summary(BreastCancer)
#Omitted Data and structure
bc_data = na.omit(BreastCancer)
dim(bc_data)
summary(bc_data)
str(bc_data)
#Basic EDA and plots
cancer_count = table(bc_data$Class)
barplot(cancer_count)
pie(cancer_count)
#basic EDA beautified
ggplot(bc_data, aes(bc_data$Class, , fill=Class)) +
geom_bar( ) +
scale_fill_hue(c = 60) +
theme(legend.position = "none") + theme_bw() +
labs(x = "Cancer type", title = "Cancer Count of Benign and Malignant", y = "Count")
#EDA scatterplot
ggplot(bc_data, aes(x=bc_data$Cell.size, y=bc_data$Cell.shape, color=Class)) +
geom_point(size=3, alpha = 0.5) + theme_bw() +
labs(x = "Cell Size", title = "Cancer Cell type", y = "Cell Shape")
#EDA of the whole dataframe with pair plots
data(bc_data)
ggpairs(bc_data, columns = 2:10, ggplot2::aes(colour=Class))
#Changing Ordinal to Numeric
bc_data$Cl.thickness= as.numeric(bc_data$Cl.thickness)
bc_data$Cell.size= as.numeric(bc_data$Cell.size)
bc_data$Cell.shape= as.numeric(bc_data$Cell.shape)
bc_data$Marg.adhesion= as.numeric(bc_data$Marg.adhesion)
bc_data$Epith.c.size= as.numeric(bc_data$Epith.c.size)
bc_data$Bare.nuclei= as.numeric(bc_data$Bare.nuclei)
bc_data$Bl.cromatin= as.numeric(bc_data$Bl.cromatin)
bc_data$Normal.nucleoli= as.numeric(bc_data$Normal.nucleoli)
bc_data$Mitoses= as.numeric(bc_data$Mitoses)
#Remove the column ID
bc_no_id = subset(bc_data, select = -c(Id))
#Applying Logistic regression
#AIC and BIC
bss_AIC = bestglm(bc_no_id, family=binomial, IC="AIC")
bss_BIC = bestglm(bc_no_id, family=binomial, IC="BIC")
#Checking the results of the subsets
bss_AIC$Subsets
bss_BIC$Subsets
#Identifying the best fitting models
(best_AIC = bss_AIC$ModelReport$Bestk)
(best_BIC = bss_BIC$ModelReport$Bestk)
#Multiple graphs
par(mfrow=c(1,2))
p = ncol(bc_no_id[1:ncol(bc_no_id)-1])
#Plotting graphs and highlighting the value of k
plot(0:p, bss_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
#AIC and BIC best fit model with 5 values
pstar = 5
## Check which predictors are in the 1-predictor model
bss_AIC$Subsets[pstar+1,]
## Intercept Lag1 Lag2 Lag3 Lag4 Lag5 Volume logLikelihood AIC
## 1 TRUE FALSE TRUE FALSE FALSE FALSE FALSE -745.2093 1492.419
## Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_AIC$Subsets[pstar+1, 2:(p+1)]))
## [1] FALSE TRUE FALSE FALSE FALSE FALSE
cancer_data = data.frame(bc_no_id[,indices])
## Obtain regression coefficients for this model
logreg1_fit = glm(Class ~ ., data=cancer_data, family="binomial")
summary(logreg1_fit)
#Rough plot for the idea of the best fit line
plot(logreg1_fit)
#LASSO regularisation technique
#Choosing the value
grid = 10^seq(-4, -1, length.out=100)
## Fit a model with LASSO penalty for each value of the tuning parameter
lasso_value = glmnet(bc_no_id[,1:9], bc_no_id$Class, family="binomial", alpha=1, standardize=FALSE, lambda=grid)
plot(lasso_value, xvar="lambda", col=rainbow(p), label=TRUE)
lasso_cv_value = cv.glmnet(as.matrix(bc_no_id[,1:9]), bc_no_id$Class, family="binomial", alpha=1, standardize=FALSE, lambda=grid,
type.measure="class")
plot(lasso_cv_value)
(lambda_lasso_min = lasso_cv_value$lambda.min)
## Find the parameter estimates associated with optimal value of the tuning parameter
coef(lasso_value, s=lambda_lasso_min)
##Applying LDA on the dataset
lda_fit = lda(Class ~ ., data = bc_no_id)
lda_fit
plot(lda_fit)
##Train and Test data
n = nrow(bc_no_id) - 1
# 10-fold cross validation...
nfolds = 10
# Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
# Print first few fold-assignments
head(fold_index)
#CV Fucntion
class_cv = function(bc_no_id, cancer_data, fold_ind, model_type, optim_lambda){
nfolds = max(fold_ind) # number of folds...
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.") # validating partitions...
cv_errors=numeric(nfolds) # empty list of size equal to number of folds...
for(fold in 1:nfolds) # fitting different models based on model_type for each fold and making prediction then finding yhat(predicted class)...
{
if(model_type == 'IC'){ # Logistic regression using features selected from subset selection AIC and BIC...
tmp_fit = glm(Class ~ ., data = cancer_data[fold_ind != fold,], family = "binomial")
pred = predict(tmp_fit, cancer_data[fold_ind == fold, 1:ncol(cancer_data)-1])
yobs = cancer_data$Class[fold_ind == fold]
yhat = ifelse(pred > 0.5, 'malignant', 'benign')
}
else if(model_type == 'LASSO'){ # LASSO Regularized Logistic Regression...
tmp_fit = glmnet(bc_no_id[fold_ind != fold, 1:9], bc_no_id[fold_ind != fold, 10], alpha = 1, family = "binomial", lambda = optim_lambda)
pred = predict(tmp_fit, as.matrix(bc_no_id[fold_ind == fold, 1:9]))
yobs = bc_no_id$Class[fold_ind == fold]
yhat = ifelse(pred > 0.5, 'malignant', 'benign')
}
else if(model_type == 'LDA'){ # Linear discriminant analysis...
tmp_fit = lda(Class ~ ., data = bc_no_id[fold_ind != fold,])
pred = predict(tmp_fit, bc_no_id[fold_ind == fold, 1:9])
yobs = bc_no_id$Class[fold_ind==fold]
yhat = ifelse(pred$x > 0.5, 'malignant', 'benign')
}
cv_errors[fold] = 1 - mean(yobs == yhat) # Calculate the training error
}
# Calculating mean error of all the folds...
fold_sizes = numeric(nfolds) # placeholder for length of each fold...
for(fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold)) # calculating length of each fold...
test_error = weighted.mean(cv_errors, w = fold_sizes) # calculating weighted mean of the cross validation errors...
return(test_error)
}
# model_type = IC for AIC, BIC logistic regression...
# model_type = LASSO for logistic regression with lasso regularization...
# model_type = LDA for Linear Discriminant Analysis...
model_type = c('IC', 'LASSO', 'LDA')
error_list = c()
for(i in model_type){
error = class_cv(bc_no_id, cancer_data, fold_index, i, lasso_cv_value$lambda.min) # performing cross validation on each type of models
error_list = append(error_list, error)
}
library(mlbench)
library(ggplot2)
library(bestglm)
library(glmnet)
library(MASS)
library(GGally)
## Load the data
data(BreastCancer)
## Check size
dim(BreastCancer)
head(BreastCancer)
#About the Dataset
?BreastCancer
summary(BreastCancer)
#Omitted Data and structure
bc_data = na.omit(BreastCancer)
dim(bc_data)
summary(bc_data)
str(bc_data)
#Basic EDA and plots
cancer_count = table(bc_data$Class)
barplot(cancer_count)
pie(cancer_count)
#basic EDA beautified
ggplot(bc_data, aes(bc_data$Class, , fill=Class)) +
geom_bar( ) +
scale_fill_hue(c = 60) +
theme(legend.position = "none") + theme_bw() +
labs(x = "Cancer type", title = "Cancer Count of Benign and Malignant", y = "Count")
#EDA scatterplot
ggplot(bc_data, aes(x=bc_data$Cell.size, y=bc_data$Cell.shape, color=Class)) +
geom_point(size=3, alpha = 0.5) + theme_bw() +
labs(x = "Cell Size", title = "Cancer Cell type", y = "Cell Shape")
#EDA of the whole dataframe with pair plots
data(bc_data)
ggpairs(bc_data, columns = 2:10, ggplot2::aes(colour=Class))
#Changing Ordinal to Numeric
bc_data$Cl.thickness= as.numeric(bc_data$Cl.thickness)
bc_data$Cell.size= as.numeric(bc_data$Cell.size)
bc_data$Cell.shape= as.numeric(bc_data$Cell.shape)
bc_data$Marg.adhesion= as.numeric(bc_data$Marg.adhesion)
bc_data$Epith.c.size= as.numeric(bc_data$Epith.c.size)
bc_data$Bare.nuclei= as.numeric(bc_data$Bare.nuclei)
bc_data$Bl.cromatin= as.numeric(bc_data$Bl.cromatin)
bc_data$Normal.nucleoli= as.numeric(bc_data$Normal.nucleoli)
bc_data$Mitoses= as.numeric(bc_data$Mitoses)
#Remove the column ID
bc_no_id = subset(bc_data, select = -c(Id))
#Applying Logistic regression
#AIC and BIC
bss_AIC = bestglm(bc_no_id, family=binomial, IC="AIC")
bss_BIC = bestglm(bc_no_id, family=binomial, IC="BIC")
#Checking the results of the subsets
bss_AIC$Subsets
bss_BIC$Subsets
#Identifying the best fitting models
(best_AIC = bss_AIC$ModelReport$Bestk)
(best_BIC = bss_BIC$ModelReport$Bestk)
#Multiple graphs
par(mfrow=c(1,2))
p = ncol(bc_no_id[1:ncol(bc_no_id)-1])
#Plotting graphs and highlighting the value of k
plot(0:p, bss_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
#AIC and BIC best fit model with 5 values
pstar = 5
## Check which predictors are in the 1-predictor model
bss_AIC$Subsets[pstar+1,]
## Intercept Lag1 Lag2 Lag3 Lag4 Lag5 Volume logLikelihood AIC
## 1 TRUE FALSE TRUE FALSE FALSE FALSE FALSE -745.2093 1492.419
## Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_AIC$Subsets[pstar+1, 2:(p+1)]))
## [1] FALSE TRUE FALSE FALSE FALSE FALSE
cancer_data = data.frame(bc_no_id[,indices])
## Obtain regression coefficients for this model
logreg1_fit = glm(Class ~ ., data=cancer_data, family="binomial")
summary(logreg1_fit)
#Rough plot for the idea of the best fit line
plot(logreg1_fit)
#LASSO regularisation technique
#Choosing the value
grid = 10^seq(-4, -1, length.out=100)
## Fit a model with LASSO penalty for each value of the tuning parameter
lasso_value = glmnet(bc_no_id[,1:9], bc_no_id$Class, family="binomial", alpha=1, standardize=FALSE, lambda=grid)
plot(lasso_value, xvar="lambda", col=rainbow(p), label=TRUE)
lasso_cv_value = cv.glmnet(as.matrix(bc_no_id[,1:9]), bc_no_id$Class, family="binomial", alpha=1, standardize=FALSE, lambda=grid,
type.measure="class")
plot(lasso_cv_value)
(lambda_lasso_min = lasso_cv_value$lambda.min)
## Find the parameter estimates associated with optimal value of the tuning parameter
coef(lasso_value, s=lambda_lasso_min)
##Applying LDA on the dataset
lda_fit = lda(Class ~ ., data = bc_no_id)
lda_fit
plot(lda_fit)
##Train and Test data
n = nrow(bc_no_id) - 1
# 10-fold cross validation...
nfolds = 10
# Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
# Print first few fold-assignments
head(fold_index)
#CV Fucntion
class_cv = function(bc_no_id, cancer_data, fold_ind, model_type, optim_lambda){
nfolds = max(fold_ind) # number of folds...
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.") # validating partitions...
cv_errors=numeric(nfolds) # empty list of size equal to number of folds...
for(fold in 1:nfolds) # fitting different models based on model_type for each fold and making prediction then finding yhat(predicted class)...
{
if(model_type == 'IC'){ # Logistic regression using features selected from subset selection AIC and BIC...
tmp_fit = glm(Class ~ ., data = cancer_data[fold_ind != fold,], family = "binomial")
pred = predict(tmp_fit, cancer_data[fold_ind == fold, 1:ncol(cancer_data)-1])
yobs = cancer_data$Class[fold_ind == fold]
yhat = ifelse(pred > 0.5, 'malignant', 'benign')
}
else if(model_type == 'LASSO'){ # LASSO Regularized Logistic Regression...
tmp_fit = glmnet(bc_no_id[fold_ind != fold, 1:9], bc_no_id[fold_ind != fold, 10], alpha = 1, family = "binomial", lambda = optim_lambda)
pred = predict(tmp_fit, as.matrix(bc_no_id[fold_ind == fold, 1:9]))
yobs = bc_no_id$Class[fold_ind == fold]
yhat = ifelse(pred > 0.5, 'malignant', 'benign')
}
else if(model_type == 'LDA'){ # Linear discriminant analysis...
tmp_fit = lda(Class ~ ., data = bc_no_id[fold_ind != fold,])
pred = predict(tmp_fit, bc_no_id[fold_ind == fold, 1:9])
yobs = bc_no_id$Class[fold_ind==fold]
yhat = ifelse(pred$x > 0.5, 'malignant', 'benign')
}
cv_errors[fold] = 1 - mean(yobs == yhat) # Calculate the training error
}
# Calculating mean error of all the folds...
fold_sizes = numeric(nfolds) # placeholder for length of each fold...
for(fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold)) # calculating length of each fold...
test_error = weighted.mean(cv_errors, w = fold_sizes) # calculating weighted mean of the cross validation errors...
return(test_error)
}
# model_type = IC for AIC, BIC logistic regression...
# model_type = LASSO for logistic regression with lasso regularization...
# model_type = LDA for Linear Discriminant Analysis...
model_type = c('IC', 'LASSO', 'LDA')
error_list = c()
for(i in model_type){
error = class_cv(bc_no_id, cancer_data, fold_index, i, lasso_cv_value$lambda.min) # performing cross validation on each type of models
error_list = append(error_list, error)
}
model_name = c('Logistic Regression with AIC, BIC', 'Logistic Regression with LASSO regularization', 'Linear Discriminant Analysis')
df_error = data.frame(model_name, error_list)
colnames(df_error) = c("Model", "Error Rate")
df_error
plot(lda_fit)
plot(lda_fit)
#Rough plot for the idea of the best fit line
plot(logreg1_fit)
View(bc_data)
rm (pstar)
rm (p)
rm (indices)
rm (cancer_count)
rm (best_AIC)
rm (best_BIC)
library('ProjectTemplate')
setwd("~/Documents/Github/Newcastle Workbook/Terapixel")
setwd("~/Documents/Github/Newcastle Workbook")
create.project('TerapixelProject')
setwd("~/Documents/Github/Newcastle Workbook/TerapixelProject/reports")
